{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pytz\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from zipline.api import symbol as zipline_symbol\n",
    "from zipline.api import order, record, fetch_csv, order_percent, order_target_percent, set_benchmark\n",
    "from zipline.algorithm import TradingAlgorithm\n",
    "from zipline.utils.factory import create_simulation_parameters\n",
    "from zipline.finance import commission\n",
    "import pyfolio as pf\n",
    "\n",
    "from query import *\n",
    "\n",
    "Start = '2008-03-11'\n",
    "End = '2020-01-10'\n",
    "\n",
    "symbols=['cotton_settle', 'cattle_settle', 'canola_settle',\n",
    "       'wheat_settle', 'copper_settle', 'corn_settle', 'oats_settle',\n",
    "       'oil_settle']\n",
    "\n",
    "symbols=['rici_settle'\n",
    "         ,'canola_settle'\n",
    "         ,'oats_settle'\n",
    "#          ,'cotton_settle'\n",
    "         ,'wheat_settle'\n",
    "         ,'corn_settle'\n",
    "         ,'copper_settle'\n",
    "#          ,'Sentiment_settle'\n",
    "         ,'oil_settle']\n",
    "#          ,'cattle_settle'\n",
    "#          ,'rica_settle']\n",
    "\n",
    "# file to store with signals\n",
    "signalsPath = getTempFile(outputPath)\n",
    "\n",
    "# 'date,close,high,low,open,volume,adjClose,adjHigh,adjLow,adjOpen,adjVolume,divCash,splitFactor'\n",
    "# columns = 'open,high,low,close,volume,adjClose'.split(\",\")\n",
    "path_to_EOD = \"/Users/frizzell/forecast_models/data/raw_data_1monthFuture2020-01-10.csv\"\n",
    "df = pd.read_csv(path_to_EOD)\n",
    "df.index =  df.Date.apply(datetime.datetime.strptime,args=('%Y-%m-%d',))\n",
    "del df[\"Date\"]\n",
    "df = df[Start:End]\n",
    "# panelPrices = getPrices(\"dumy\", symbols, panelStart, panelEnd, columns)\n",
    "# panelPrices.minor_axis = columns\n",
    "dateToStringF = lambda x: x.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail(10)#09\n",
    "# print(signalsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail(10)#07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some functions\n",
    "# def zScore(series,span): \n",
    "#     ts = ( series - series.ewm(span).mean() ) / series.ewm(span).std()\n",
    "#     return ( ts - ts.mean() ) / ts.std(ddof=0)\n",
    "# def returns(series,future_span):\n",
    "#     return -1.0 * (series - series.rolling(future_span).mean().shift(-1*future_span)) / series.rolling(future_span).std()\n",
    "\n",
    "# 1monthfutures_20100101_20170101_20180101_forward-2_back-30_lmbda-1e-09_t_shape-uniform_f_shape-uniform_lstm-1_batch_size50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = pd.to_datetime('2008-01-01', format='%Y-%m-%d', errors='ignore')\n",
    "train_end = pd.to_datetime('2017-01-01', format='%Y-%m-%d', errors='ignore')\n",
    "test_start = pd.to_datetime('2018-01-01', format='%Y-%m-%d', errors='ignore')\n",
    "test_end = pd.to_datetime('2020-01-10', format='%Y-%m-%d', errors='ignore')\n",
    "# test_end = pd.to_datetime('2019-06-30', format='%Y-%m-%d', errors='ignore')\n",
    "\n",
    "\n",
    "# 1monthfuturesforward-15_back-15_lmbda-1e-09_t_shape-uniform_f_shape-uniform_batch_size10\n",
    "# look_forward=300\n",
    "# look_back=700\n",
    "\n",
    "look_forward=2\n",
    "look_back=30\n",
    "\n",
    "def create_features_old(df,train_start_date,test_start_date,test_end_date,back=30,forward=30):\n",
    "    X = df.drop(columns=[col for col in df.columns])\n",
    "    no_repeat = []\n",
    "    for col_1 in df:\n",
    "        for col_2 in df:\n",
    "            if col_1 == col_2:\n",
    "                continue\n",
    "            no_repeat.append(col_1)\n",
    "            if col_2 in no_repeat:\n",
    "                continue\n",
    "            X[col_1.split('_')[0]+'-'+col_2.split('_')[0]] = df[col_1]-df[col_2]\n",
    "#     X = X-X.rolling(back).mean()\n",
    "#     X = X-X.mean()\n",
    "    production_date = X[:test_end_date].index[-1*forward] \n",
    "            \n",
    "    return X[train_start_date:test_start_date], \\\n",
    "           X[test_start_date:production_date], \\\n",
    "           X[production_date:]\n",
    "\n",
    "def create_features(df,train_start_date,train_end_date,test_start_date,test_end_date,back=30,forward=30):\n",
    "    X = df.drop(columns=[col for col in df.columns])\n",
    "    no_repeat = []\n",
    "    for col_1 in df:\n",
    "        for col_2 in df:\n",
    "            if col_1 == col_2:\n",
    "                continue\n",
    "            no_repeat.append(col_1)\n",
    "            if col_2 in no_repeat:\n",
    "                continue\n",
    "            X[col_1.split('_')[0]+'-'+col_2.split('_')[0]] = df[col_1]-df[col_2]\n",
    "#     X = X-X.rolling(back).mean()\n",
    "#     X = X-X.mean()\n",
    "    production_date = X[:test_end_date].index[-1*forward] \n",
    "            \n",
    "    return X[train_start_date:train_end_date], \\\n",
    "           X[test_start_date:production_date], \\\n",
    "           X[production_date:]\n",
    "\n",
    "def make_targets_old(df,train_start_date,test_start_date,test_end_date,forward=30):\n",
    "    Y = -1.0*(df.wheat_settle - df.wheat_settle.rolling(forward).mean().shift(-1*forward))\n",
    "    \n",
    "    production_date = Y[:test_end_date].index[-1*forward] \n",
    "    \n",
    "    return Y[train_start_date:test_start_date],Y[train_start_date:test_start_date].index, \\\n",
    "           Y[test_start_date:production_date],Y[test_start_date:production_date].index\n",
    "\n",
    "def make_targets(df,train_start_date,train_end_date,test_start_date,test_end_date,forward=30):\n",
    "    \n",
    "    Y = -1.0*(df.wheat_settle - df.wheat_settle.rolling(forward).mean().shift(-1*forward))\n",
    "    \n",
    "    production_date = Y[:test_end_date].index[-1*forward] \n",
    "    \n",
    "    return Y[train_start_date:train_end_date],Y[train_start_date:train_end_date].index, \\\n",
    "           Y[test_start_date:production_date],Y[test_start_date:production_date].index\n",
    "\n",
    "\n",
    "Y_train,Y_train_dates,Y_test,Y_test_dates = make_targets(df,\n",
    "                                                  train_start,\n",
    "                                                  train_end,\n",
    "                                                  test_start,\n",
    "                                                  test_end,\n",
    "                                                  forward=look_forward)\n",
    "\n",
    "X_train, X_test, X_production = create_features(df,\n",
    "                                                train_start,\n",
    "                                                train_end,\n",
    "                                                test_start,\n",
    "                                                test_end,\n",
    "                                                back=look_back,\n",
    "                                                forward=look_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "feature_transformer = QuantileTransformer(n_quantiles=1000,\n",
    "                                        output_distribution='uniform',\n",
    "                                        copy=True)\n",
    "\n",
    "target_transformer = QuantileTransformer(n_quantiles=1000,\n",
    "                                        output_distribution='uniform',\n",
    "                                        copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# # DF = feature_transformer.fit_transform(DF)X\n",
    "# X_train = feature_transformer.fit_transform(X_train)\n",
    "# X_test = feature_transformer.transform(X_test)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# DF = feature_transformer.fit_transform(DF)X\n",
    "X_train = feature_transformer.fit_transform(X_train)\n",
    "Y_train = target_transformer.fit_transform(Y_train.values.reshape(-1,1))\n",
    "X_test = feature_transformer.transform(X_test)\n",
    "Y_test = target_transformer.transform(Y_test.values.reshape(-1,1))\n",
    "print(Y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_test.shape)\n",
    "# num_var = X_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the regressors for the model\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, LSTM, Dense\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_gen = TimeseriesGenerator(X_test,\n",
    "#                                      X_test[:,0] ,\n",
    "#                                      length=look_back,\n",
    "#                                      batch_size=len(X_test),\n",
    "#                                      reverse=False)\n",
    "# # print(train_gen[0][0].shape)\n",
    "# print(test_gen[0][0].shape)\n",
    "# print(test_gen[0][0].shape[2])\n",
    "\n",
    "train_gen = TimeseriesGenerator(X_train,\n",
    "                                Y_train ,\n",
    "                                length=look_back,\n",
    "                                batch_size=int(len(X_train)/1),\n",
    "                                reverse=False)\n",
    "\n",
    "test_gen = TimeseriesGenerator(X_test,\n",
    "                                     Y_test ,\n",
    "                                     length=look_back,\n",
    "                                     batch_size=len(X_test),\n",
    "                                     reverse=False)\n",
    "print(train_gen[0][0].shape)\n",
    "print(test_gen[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "def loss_wrapper(beta):\n",
    "    _beta = beta\n",
    "    def customLoss(y_true,y_pred):\n",
    "        if not K.is_tensor(y_pred):\n",
    "            y_pred = K.constant(y_pred)\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "#         print(y_true)\n",
    "#         K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "        return K.mean(K.square(y_pred - y_true), axis=-1) + K.maximum(0.0,_beta*K.mean(-y_pred * y_true, axis=-1))\n",
    "\n",
    "    return customLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# custom_objects={'loss_max': loss_max}\n",
    "# folder = '1monthfuturesforward-2_back-30_lmbda-1e-13_t_shape-uniform_f_shape-uniform_batch_size50'\n",
    "folder = '1monthfutures_20100101_20170101_20180101_forward-2_back-30_lmbda-1e-09_t_shape-uniform_f_shape-uniform_lstm-1_batch_size50'\n",
    "# folder = 'customLoss_1monthfutures_20100101_20170101_20180101_forward-2_back-30_lmbda-1e-10_t_shape-uniform_f_shape-uniform_lstm-2_batch_size100'\n",
    "# folder = '1monthfuturesforward-15_back-15_lmbda-1e-09_t_shape-uniform_f_shape-uniform_batch_size10'\n",
    "file = 'model-epoch073-loss0.0499.hdf5'\n",
    "# file = 'model-epoch001-loss0.0503.hdf5'#Best\n",
    "# file = 'model-epoch108-loss0.0498.hdf5'\n",
    "# file = 'model-epoch182-loss0.0612.hdf5'\n",
    "# file ='model-epoch083-loss0.0464.hdf5'\n",
    "model = load_model(\"/Users/frizzell/forecast_models/{0}/{1}\".format(folder,file),custom_objects={'customLoss': loss_wrapper(0.5)})\n",
    "\n",
    "# model = load_model(\"/Users/frizzell/forecast_models/forward-5_back-5_lmbda-1e-09_t_shape-uniform_f_shape-uniform_batch_size50/model-epoch209-loss0.0523.hdf5\")\n",
    "# model = load_model(\"/Users/frizzell/forecast_models/all_variables_models/forward30_back30_model-034-0.5167.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(7)\n",
    "# num_of_features = test_gen[0][0].shape[2]\n",
    "# look_back=90\n",
    "# window_size = look_back\n",
    "\n",
    "# # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(num_of_features*2,kernel_regularizer=regularizers.l1(0.0000001) ,\n",
    "#                activity_regularizer=regularizers.l1(0.0000001),\n",
    "#                return_sequences=True, input_shape=(window_size, num_of_features)))  # returns a sequence of vectors of dimension 32\n",
    "# #model.add(SimpleRNN(num_of_features*2, return_sequences=True, input_shape=(window_size, num_of_features)))  # returns a sequence of vectors of dimension 32\n",
    "# #model.add(LSTM(num_of_features*2, return_sequences=True))\n",
    "# #model.add(LSTM(num_of_features , return_sequences=True))\n",
    "# model.add(SimpleRNN(num_of_features,kernel_regularizer=regularizers.l2(0.0000001),\n",
    "#                     activity_regularizer=regularizers.l1(0.0000001)))\n",
    "# model.add(Dense(num_of_features,kernel_regularizer=regularizers.l2(0.0000001),\n",
    "#                 activity_regularizer=regularizers.l1(0.0000001), activation='selu',))\n",
    "# model.add(Dense(int(num_of_features/2),kernel_regularizer=regularizers.l2(0.0000001),\n",
    "#                 activity_regularizer=regularizers.l1(0.0000001), activation='tanh'))\n",
    "# model.add(Dense(2, activation='tanh',activity_regularizer=regularizers.l1(0.0000001)))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "# #sgd = SGD(lr=0.01, decay=1e-8, momentum=0.4, nesterov=True)\n",
    "# #loss='mean_squared_error', optimizer='adam'\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "#               #optimizer=sgd,\n",
    "#               metrics= [metrics.mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and train our models\n",
    "# students = build_regressors(y_train.columns)\n",
    "# smart_students, train_predictions = train_regressors(students,x_train,y_train)\n",
    "# model_weights = \"/Users/frizzell/forecast_models/back90_forward30_rnn.hdf\"\n",
    "# model.load_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPI = np.full((len(X_test),1),np.nan)\n",
    "for x in range(look_back,len(X_test)):\n",
    "    \n",
    "    feature = X_test[x-look_back:x]\n",
    "    KPI[x] =  model.predict( feature[np.newaxis,...] )\n",
    "\n",
    "KPI_flat = KPI[:,0]\n",
    "\n",
    "# KPI_mean = KPI_flat[~np.isnan(KPI_flat)].mean()\n",
    "# KPI_std = KPI_flat[~np.isnan(KPI_flat)].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = target_transformer.inverse_transform( KPI_flat.reshape(-1,1))# / KPI_std\n",
    "# plt.hist(indicators)\n",
    "# plt.show()\n",
    "# indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indicators.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Dirty test to see what we are learning. \n",
    "#Essentially ifthe histogram is shaper, narrower, and more centered at zero it means we are more correct\n",
    "#than if we simply guess the price in X days is the same as it is now\n",
    "# ave = 4\n",
    "# pred = pd.DataFrame(p).rolling(ave).mean()[ave:]\n",
    "# plt.hist(t-p,bins=50)\n",
    "# plt.show()\n",
    "# print((t-p).mean())\n",
    "# print((t-p).std())\n",
    "# # print(t-p)\n",
    "# for x in range(0,len(pred)):\n",
    "#     if abs(pred[x]) > 0.5:\n",
    "#         print(str(pred[x])+\" \" +str(t[x]-p[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets have a peak at the regression predictions and make sure there are approximatly centered at zero\n",
    "# for symbol in symbols:\n",
    "#     short_plot(smart_students[symbol].predict(x_test) / train_predictions[symbol].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest = df.iloc[-len(indicators):]\n",
    "backtest[\"rnn_score\"] = indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the dataframe panel to use for backtesting.\n",
    "dataframes = []\n",
    "panel_dictionary = {}\n",
    "# pdb.set_trace()\n",
    "for symbol in symbols:\n",
    "    rnn_score = backtest[\"rnn_score\"] \n",
    "#     df[symbol+\" MovAveScore\"] = signalMovAvrZscore.to_frame()\n",
    "    temp = rnn_score.to_frame().dropna()\n",
    "    temp.columns     = [\"rnn_score\"]\n",
    "    temp[\"wheat_MA20\"] = (backtest[\"wheat_settle\"].rolling(2).mean() - backtest[\"wheat_settle\"].mean() )/backtest[\"wheat_settle\"].std()# Renormalize to gaussian using training std\n",
    "#     df[\"VolumeScore\"] = zScore( testSet[symbol]['volume'] ,10)\n",
    "    temp[\"Symbol\"]   = symbol\n",
    "    temp[\"close\"] = backtest[symbol]\n",
    "    temp[\"date\"]     = temp.index#.map(dateToStringF)\n",
    "    dataframes.append(temp)\n",
    "    panel_dictionary[symbol] = temp\n",
    "\n",
    "    \n",
    "do_spy = True\n",
    "if do_spy:\n",
    "    base = \"/Users/frizzell/forecast_models/data\"\n",
    "    benchmark = pd.read_csv(base+\"/benchmark_data_2020-01-10.csv\")\n",
    "    benchmark.index = pd.DatetimeIndex(benchmark.date)\n",
    "    benchmark.index = pd.DatetimeIndex([datetime.datetime.strptime(str(x).split(\"T\")[0],'%Y-%m-%d') for x in benchmark.index.values ])\n",
    "    benchmark = benchmark.loc[temp.index]\n",
    "    benchmark = benchmark[ ['adjClose','adjOpen'] ]\n",
    "#     benchmark['change'] = (benchmark['adjClose']- benchmark['adjClose'].shift(1)).fillna(0)\n",
    "    benchmark['change'] = benchmark['adjClose'].pct_change().fillna(0)#(benchmark['adjClose']- benchmark['adjClose'].shift(1)).fillna(0)\n",
    "    benchmark[\"close\"] = benchmark['adjClose']\n",
    "    benchmark[\"open\"] = benchmark[\"adjOpen\"]\n",
    "    benchmark[\"date\"] = benchmark.index\n",
    "    benchmark[\"Symbol\"] = \"benchmark\"\n",
    "    benchmark = benchmark.interpolate(method=\"time\")\n",
    "    panel_dictionary[\"benchmark\"] = benchmark\n",
    "\n",
    "result = pd.concat(dataframes)\n",
    "backtestPanel = pd.Panel(panel_dictionary)\n",
    "result.to_csv(signalsPath,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtestPanel[\"benchmark\"]\n",
    "# print(benchmark.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backtestPanel[\"wheat_settle\"].rnn_score.std()) #old)\n",
    "\n",
    "rnn_std = backtestPanel[\"wheat_settle\"].rnn_score.std()\n",
    "\n",
    "backtestPanel[\"wheat_settle\"].head(3) #old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['VolumeScore'].hist(bins=30)\n",
    "# plt.show()\n",
    "# result\n",
    "# backtestPanel[\"wheat_settle\"]\n",
    "# backtestPanel[\"benchmark\"]\n",
    "# Asset('NYSE',)\n",
    "# backtestPanel[\"benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.assets import Asset\n",
    "SignalThreshold = rnn_std/2# sigma threshold for signal to open position\n",
    "AveThreshold = 0.0\n",
    "\n",
    "close_parameter = rnn_std/2\n",
    "from six import viewkeys\n",
    "\n",
    "# load signal to Zipline\n",
    "def initialize(context):\n",
    "    df = fetch_csv(signalsPath, symbol_column=\"Symbol\")\n",
    "    set_benchmark( zipline_symbol(\"benchmark\") )\n",
    "    context.set_commission(commission.PerShare(cost=0.0, min_trade_cost=0))\n",
    "    \n",
    "#     print(context)\n",
    "#     print(dir(context))\n",
    "#     print(context.symbols)\n",
    "    #df = pd.read_csv(signalsPath)#, symbol_column=\"Symbol\")\n",
    "\n",
    "\n",
    "    #print(strLocation()+str(df.df.head()))\n",
    "    \n",
    "# write simple signal-based logic - no hedge , longOnly\n",
    "# open at z=-K, close at z=0 or z=-2*K \n",
    "def handle_correlations_aggressively(context, data):\n",
    "    # check for all symbols signal value\n",
    "    \n",
    "    portfolio=context.portfolio\n",
    "    record(positions=len(portfolio.positions.keys()))\n",
    "    # print(\"Date = \"+str(algo_obj.get_datetime()))\n",
    "    \n",
    "    for symb in [\"wheat_settle\",]: \n",
    "\n",
    "        sec = zipline_symbol(symb)\n",
    "#         debug(sec)\n",
    "        if (data[sec].rnn_score > SignalThreshold) and (data[sec].wheat_MA20 < -AveThreshold) and (sec not in viewkeys(portfolio.positions)):\n",
    "#         if (data[sec].rnn_score > SignalThreshold) and (sec not in viewkeys(portfolio.positions)):\n",
    "\n",
    "\n",
    "            #for asset in viewkeys(portfolio.positions):\n",
    "            order_target_percent(sec, 0)\n",
    "            order_target_percent(sec, 1.0) # long\n",
    "            print(\"Lets buy \"+symb+\"  \"+str(algo_obj.get_datetime()))\n",
    "            \n",
    "        if (data[sec].rnn_score < -SignalThreshold) and (data[sec].wheat_MA20 > AveThreshold) and (sec not in viewkeys(portfolio.positions)):\n",
    "#         if (data[sec].rnn_score < -SignalThreshold) and (sec not in viewkeys(portfolio.positions)):\n",
    "\n",
    "            #for asset in viewkeys(portfolio.positions):\n",
    "            order_target_percent(sec, 0)\n",
    "            order_target_percent(sec, -1.0) # full portfolio loaded with symb\n",
    "            print(\"Lets short \"+symb+\"  \"+str(algo_obj.get_datetime()))\n",
    "            \n",
    "\n",
    "        \n",
    "    for asset in viewkeys(portfolio.positions):\n",
    "        shares = portfolio.positions[asset].amount \n",
    "        #if (abs(data[asset].MVAscore) < close_parameter) : # close position logic - cross 0 or StopLoss\n",
    "        if  ((shares > 0) and (data[asset].rnn_score < close_parameter)) \\\n",
    "        or  ((shares < 0) and (data[asset].rnn_score > -1.0*close_parameter)) : # close position logic - cross 0 or StopLoss\n",
    "            order_target_percent(asset, 0)\n",
    "            print(\"Lets close \"+str(asset)+\"  \"+str(algo_obj.get_datetime()))  \n",
    "\n",
    "\n",
    "def buy_hold(context, data):\n",
    "    \n",
    "    portfolio=context.portfolio\n",
    "    record(positions=len(portfolio.positions.keys()))\n",
    "    for symb in [\"CXO\"]:\n",
    "        sec = zipline_symbol(symb)\n",
    "        #order_target_percent(sec, 0 ) # full portfolio loaded with symb\n",
    "        order_target_percent(sec, 1) # full portfolio loaded with symb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtestPanel[\"benchmark\"]['close'].tail(5)\n",
    "# print( algo_obj.trading_calendar.schedule[str(start.date()):str(end.date())])\n",
    "\n",
    "# str(start.date())\n",
    "# backtestPanel[\"benchmark\"][  ]\n",
    "# algo_obj.trading_calendar.schedule[str(start.date()):str(end.date())].index\n",
    "# benchmark_change.index\n",
    "# backtestPanel[\"wheat_settle\"].index\n",
    "# backtestPanel[\"benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "start = datetime.datetime(2018, 2, 14, 0, 0, 0, 0, pytz.utc)\n",
    "end = datetime.datetime(2020, 1, 10, 0, 0, 0, 0, pytz.utc)\n",
    "# end = datetime.datetime(2019, 6, 30, 0, 0, 0, 0, pytz.utc)\n",
    "\n",
    "\n",
    "\n",
    "sim_params = create_simulation_parameters(start=start,\n",
    "                                          end=end,\n",
    "                                          capital_base=float(\"100000\"),\n",
    "                                          data_frequency='daily',\n",
    "                                          trading_calendar=None)\n",
    "\n",
    "print(\"Starting simulation with \")\n",
    "# print(sim_params)\n",
    "algo_obj = TradingAlgorithm(initialize=initialize,\n",
    "                            handle_data=handle_correlations_aggressively,\n",
    "                            sim_params=sim_params)\n",
    "\n",
    "benchmark_change = backtestPanel[\"benchmark\"][[ str(x.date())  in algo_obj.trading_calendar.schedule[str(start.date()):str(end.date())].index for x in backtestPanel[\"benchmark\"].index]]\n",
    "\n",
    "# algo_obj.benchmark_hack = benchmark_change[\"change\"]\n",
    "\n",
    "perf_manual = algo_obj.run(backtestPanel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(perf_manual.benchmark_period_return.head(6))\n",
    "# perf_manual.benchmark_period_return\n",
    "perf_manual.benchmark_period_return.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['2017-01-03']\n",
    "# history = pf.utils.extract_rets_pos_txn_from_zipline(pd.DataFrame(perf_manual))\n",
    "# # history\n",
    "# b = perf_manual.benchmark_period_return\n",
    "# c = backtestPanel[\"benchmark\"][\"change\"]\n",
    "# for x in range(0,len(b)):\n",
    "#     print( str(b[x])+\"---\"+str(c[x]) )\n",
    "# perf_manual.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_manual.portfolio_value.plot()\n",
    "perf_manual.returns.sum()\n",
    "# perf_manual.portfolio_value[0]\n",
    "# benchmark_offset\n",
    "# spy = backtestPanel[\"benchmark\"].close\n",
    "# 100000 + backtestPanel[\"benchmark\"].close.pct_change().map(lambda x:x*100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "plt.clf()\n",
    "\n",
    "fig, axs = plt.subplots(5)\n",
    "\n",
    "fig.set_size_inches(16, 30, forward=True)\n",
    "\n",
    "axs[0].plot( perf_manual.portfolio_value, label='Portfolio Value')\n",
    "axs[0].set_ylabel('Portfolio Value', color='b')\n",
    "\n",
    "\n",
    "axs02 = axs[0].twinx() \n",
    "axs02.plot( backtestPanel.benchmark.close , label='SPY+',color='r')\n",
    "axs02.set_ylim(233,649)\n",
    "axs02.set_ylabel('SPY', color='r')\n",
    "axs[0].legend()\n",
    "axs02.legend(loc='lower right')\n",
    "axs[0].set_xlim([datetime.date(2018, 3, 1),datetime.date(2020, 1, 21)])\n",
    "axs[0].set_ylim([90000,250000])\n",
    "# plt.show()\n",
    "# plt.subplot(1,5,2)\n",
    "\n",
    "\n",
    "# axs[1].figure().set_size_inches(16, 6, forward=True)\n",
    "axs[1].plot( backtestPanel[\"wheat_settle\"].close , label='Market Price' )\n",
    "\n",
    "axs[1].legend()\n",
    "axs[1].set_xlim([datetime.date(2018, 3, 1),datetime.date(2020, 1, 21)])\n",
    "# plt.show()\n",
    "# plt.subplot(1,5,3)\n",
    "\n",
    "\n",
    "# axs[2].figure().set_size_inches(16, 6, forward=True)\n",
    "axs[2].plot( backtestPanel[\"wheat_settle\"].rnn_score, label='RNN Score')\n",
    "axs[2].plot( backtestPanel[\"wheat_settle\"].wheat_MA20, label='MA20')\n",
    "axs[2].legend()\n",
    "axs[2].set_xlim([datetime.date(2018, 3, 1),datetime.date(2020, 1, 21)])\n",
    "# plt.show()\n",
    "# plt.subplot(1,5,4)\n",
    "\n",
    "\n",
    "\n",
    "# axs[3].figure().set_size_inches(16, 6, forward=True)\n",
    "axs[3].plot( perf_manual.ending_value, label='Position Value')\n",
    "axs[3].legend()\n",
    "axs[3].set_xlim([datetime.date(2018, 3, 1),datetime.date(2020, 1, 21)])\n",
    "axs[3].set_ylim([-270000,270000])\n",
    "# plt.show()\n",
    "# plt.subplot(1,5,5)\n",
    "\n",
    "# axs[4].figure().set_size_inches(16, 6, forward=True)\n",
    "axs[4].plot( perf_manual.positions, label='# of Positions')\n",
    "axs[4].legend()\n",
    "axs[4].set_xlim([datetime.date(2018, 3, 1),datetime.date(2020, 1, 21)])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_manual.alpha.plot()\n",
    "print(perf_manual.alpha.mean())\n",
    "plt.plot(perf_manual.alpha.rolling(100).mean().dropna())\n",
    "plt.show()\n",
    "perf_manual.beta.plot()\n",
    "print(perf_manual.beta.mean())\n",
    "plt.plot(perf_manual.beta.rolling(100).mean().dropna())\n",
    "plt.show()\n",
    "# pf.create_simple_tear_sheet(perf_manual.returns)\n",
    "# returns, positions, transactions = pf.utils.extract_rets_pos_txn_from_zipline(perf_manual.returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.create_full_tear_sheet(perf_manual.returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = [ x for x in perf_manual.transactions if x != [] ]\n",
    "# trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_summary = [] \n",
    "for x in range(0,len(trades),2):\n",
    "    if x == len(trades)-1:\n",
    "        break\n",
    "    enter = trades[x][0]\n",
    "    exit = trades[x+1][0]\n",
    "    trades_summary.append(-1.0*(enter[\"amount\"] * enter[\"price\"] + exit[\"amount\"] * exit[\"price\"]))\n",
    "    \n",
    "# trades_summary = perf_manual.returns.values\n",
    "print( sum(trades_summary) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trades_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_summary = np.asarray(trades_summary)\n",
    "plt.hist(trades_summary,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_throws = 1000000\n",
    "\n",
    "bootstraps = np.full((num_throws,len(trades_summary)),np.nan)\n",
    "\n",
    "for x in range(0,num_throws):\n",
    "    \n",
    "    bootstraps[x] = np.random.choice(trades_summary,len(trades_summary))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bootstraps[:,-1],bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_drawdown(array):\n",
    "    accumulator = 0 \n",
    "    dips = [] \n",
    "    for x in array:\n",
    "        if x < 0:\n",
    "            accumulator = accumulator + x\n",
    "        else:\n",
    "            dips.append( abs(accumulator) )\n",
    "            accumulator = 0\n",
    "#     print(dips)\n",
    "    return max(dips)\n",
    "\n",
    "draw_downs = []\n",
    "\n",
    "for sampling in bootstraps:\n",
    "    draw_downs.append( max_drawdown(sampling) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(draw_downs,bins=30)\n",
    "plt.show()\n",
    "# draw_downs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MC(transactions):\n",
    "    init = 100000\n",
    "    simulation = [init,]\n",
    "\n",
    "    for x in transactions:\n",
    "        simulation.append(init+x)\n",
    "        init = init+x\n",
    "    return simulation\n",
    "\n",
    "for i,strap in enumerate(bootstraps):\n",
    "    if not i % 4000:\n",
    "        plt.plot(make_MC(strap))\n",
    "    \n",
    "    \n",
    "plt.plot(make_MC(trades_summary), color='black', linewidth=3)\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"# Trades\")\n",
    "plt.title(\"Monte Carlo Simulation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d for d in draw_downs if d != 0 ]\n",
    "len(draw_downs)\n",
    "short = [d for d in draw_downs if not d % 10 ]\n",
    "len(short)\n",
    "# draw_downs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "plt.clf()\n",
    "print(scipy.stats.lognorm._fitstart(draw_downs))\n",
    "muX = np.mean(np.log([d+1 for d in draw_downs]))\n",
    "# sigmaX = np.std(np.log(draw_downs))\n",
    "print(muX)\n",
    "shape,loc,scale = scipy.stats.lognorm.fit([d for d in draw_downs if d != 0 ],floc=0,scale=6000)#,scale=muX)\n",
    "linespace = np.linspace(0,40000,10000)\n",
    "\n",
    "print( np.asarray(draw_downs).mean())\n",
    "print( np.asarray(draw_downs).std())\n",
    "print(shape,loc,scale)\n",
    "\n",
    "\n",
    "pdf = scipy.stats.lognorm.pdf(linespace, shape, loc, scale)\n",
    "\n",
    "# dist = scipy.stats.lognorm([stddev],loc=mean)\n",
    "\n",
    "plt.hist( draw_downs, bins = 10 ,normed=True)\n",
    "plt.plot(linespace, pdf )\n",
    "# plt.plot(linespace,lognorm.pdf(linespace))\n",
    "# shape, floc, scale\n",
    "plt.show()\n",
    "cdf = scipy.stats.lognorm.cdf(linespace, shape, loc, scale)\n",
    "plt.plot( linespace, cdf ) \n",
    "\n",
    "conf = 0\n",
    "for x,num in zip(linespace,cdf):\n",
    "    if num > .95:\n",
    "        print(\"p val = 0.95 at : \" + str(x))\n",
    "        conf = x\n",
    "        break\n",
    "        \n",
    "# plt.plot(conf,[1,])\n",
    "plt.axvline(x=conf, color='k', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
